{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d",
   "display_name": "Python 3.9.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Full Archive Search\n",
    "\n",
    "- Coppying form this github page https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Full-Archive-Search/full-archive-search.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import keys from file not commited to GitHub to keep my credentials secret\n",
    "from setup_api import bearer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up functions needed for search\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n"
   ]
  },
  {
   "source": [
    "# Turn into function for a given user\n",
    "\n",
    "Got it to work here, try to functionise it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all results from one user\n",
    "\n",
    "# TODO: Documentation\n",
    "def users_tweets(user, start_time, end_time):\n",
    "    '''\n",
    "    users - a list of twitter user\n",
    "    start_time - start date + time we want #TODO: Implement\n",
    "    end_time - end date + time we want   #TODO: Implement\n",
    "    '''\n",
    "    num_runs = 0\n",
    "    all_results = []\n",
    "\n",
    "    result_count = 100  # initilise this as a non-zero number\n",
    "                        # will be filled with respnses from json later \n",
    "                        # can get a max of 100 records at a time\n",
    "                        # so if the results arnt 100 we ran out of things to get\n",
    "\n",
    "    while result_count == 100:\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        if num_runs ==0:\n",
    "            # No next_token parameter\n",
    "            query_params = { #\"query\":\"from:CNNChile lang:es\",\n",
    "                    'query': f'(from:{user} -is:retweet) ',\n",
    "                    'tweet.fields': 'author_id,id,created_at',\n",
    "                    \"start_time\":\"2019-10-01T07:20:50.52Z\",\n",
    "                    \"end_time\":\"2019-11-01T07:20:50.52Z\",\n",
    "                    \"max_results\": \"100\"\n",
    "            }\n",
    "        else:\n",
    "            # with next_token\n",
    "            query_params = { #\"query\":\"from:CNNChile lang:es\",\n",
    "                    'query': f'(from:{user} -is:retweet) ',\n",
    "                    'tweet.fields': 'author_id,id,created_at',\n",
    "                    \"start_time\":f\"{start_time}\",\n",
    "                    \"end_time\":f\"{end_time}\",\n",
    "                    \"max_results\": \"100\",\n",
    "                    \"next_token\": results_json['meta']['next_token'] # get token from before: 'b26v89c19zqg8o3fnm91i40gu1gp7vh7j0r6ddrjyhvul'\n",
    "            }\n",
    "\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "        results = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "\n",
    "        results_json = json.loads(results)\n",
    "\n",
    "        # append this pagnation to all results\n",
    "        #all_results.append(results_json)\n",
    "        result_count = results_json['meta']['result_count']\n",
    "        num_runs = num_runs + 1\n",
    "        #print(num_runs, result_count)\n",
    "\n",
    "        # Only add results if theyre non zero\n",
    "        if result_count > 0:\n",
    "            list_of_tweet_dicts = results_json['data']\n",
    "\n",
    "            print(f\"Tweet {len(all_results)} from {list_of_tweet_dicts[0]['created_at']}\")\n",
    "\n",
    "            for tweet in list_of_tweet_dicts:\n",
    "                all_results.append([user, tweet['created_at'], tweet['id']])\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\nTweet 0 from 2019-10-31T19:37:00.000Z\nWall time: 1.56 s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['nigel_farage', '2019-10-31T19:37:00.000Z', '1189989669620080641'],\n",
       " ['nigel_farage', '2019-10-31T18:07:33.000Z', '1189967159176388611'],\n",
       " ['nigel_farage', '2019-10-31T16:58:43.000Z', '1189949832821137408'],\n",
       " ['nigel_farage', '2019-10-31T11:08:41.000Z', '1189861745009053697'],\n",
       " ['nigel_farage', '2019-10-29T20:44:06.000Z', '1189281779603124227'],\n",
       " ['nigel_farage', '2019-10-29T18:34:42.000Z', '1189249212447215616'],\n",
       " ['nigel_farage', '2019-10-29T18:06:55.000Z', '1189242221121495041'],\n",
       " ['nigel_farage', '2019-10-28T19:18:00.000Z', '1188897721085771777'],\n",
       " ['nigel_farage', '2019-10-27T18:58:28.000Z', '1188530419635052551'],\n",
       " ['nigel_farage', '2019-10-27T17:23:11.000Z', '1188506439742119939'],\n",
       " ['nigel_farage', '2019-10-27T10:22:53.000Z', '1188400668983414786'],\n",
       " ['nigel_farage', '2019-10-26T22:29:02.000Z', '1188221022601990145'],\n",
       " ['nigel_farage', '2019-10-26T14:46:07.000Z', '1188104526911496192'],\n",
       " ['nigel_farage', '2019-10-26T11:55:50.000Z', '1188061673992667137'],\n",
       " ['nigel_farage', '2019-10-25T08:30:10.000Z', '1187647526285008896'],\n",
       " ['nigel_farage', '2019-10-23T17:05:27.000Z', '1187052427268284416'],\n",
       " ['nigel_farage', '2019-10-23T09:14:55.000Z', '1186934014864691200'],\n",
       " ['nigel_farage', '2019-10-22T18:56:24.000Z', '1186717959173165057'],\n",
       " ['nigel_farage', '2019-10-22T17:04:51.000Z', '1186689886008545287'],\n",
       " ['nigel_farage', '2019-10-22T11:06:44.000Z', '1186599766362341376'],\n",
       " ['nigel_farage', '2019-10-22T07:35:55.000Z', '1186546710467764224'],\n",
       " ['nigel_farage', '2019-10-21T16:06:25.000Z', '1186312793819090945'],\n",
       " ['nigel_farage', '2019-10-21T15:51:17.000Z', '1186308984266997766'],\n",
       " ['nigel_farage', '2019-10-20T08:25:49.000Z', '1185834491866243074'],\n",
       " ['nigel_farage', '2019-10-19T16:54:06.000Z', '1185600020558548992'],\n",
       " ['nigel_farage', '2019-10-19T13:15:43.000Z', '1185545061028761601'],\n",
       " ['nigel_farage', '2019-10-18T19:07:16.000Z', '1185271144007569408'],\n",
       " ['nigel_farage', '2019-10-18T18:40:20.000Z', '1185264366809161728'],\n",
       " ['nigel_farage', '2019-10-18T18:26:43.000Z', '1185260938947108864'],\n",
       " ['nigel_farage', '2019-10-18T18:20:18.000Z', '1185259325234860032'],\n",
       " ['nigel_farage', '2019-10-18T17:18:05.000Z', '1185243665532370946'],\n",
       " ['nigel_farage', '2019-10-17T14:14:05.000Z', '1184834973662175234'],\n",
       " ['nigel_farage', '2019-10-17T11:16:36.000Z', '1184790307164037121'],\n",
       " ['nigel_farage', '2019-10-17T07:43:43.000Z', '1184736733696577536'],\n",
       " ['nigel_farage', '2019-10-16T16:34:18.000Z', '1184507872094363648'],\n",
       " ['nigel_farage', '2019-10-15T19:26:50.000Z', '1184188904099979265'],\n",
       " ['nigel_farage', '2019-10-15T17:31:59.000Z', '1184159999779586050'],\n",
       " ['nigel_farage', '2019-10-15T15:47:04.000Z', '1184133597298593799'],\n",
       " ['nigel_farage', '2019-10-14T19:18:00.000Z', '1183824294973001732'],\n",
       " ['nigel_farage', '2019-10-14T08:24:16.000Z', '1183659775961288704'],\n",
       " ['nigel_farage', '2019-10-13T17:52:11.000Z', '1183440307695820801'],\n",
       " ['nigel_farage', '2019-10-13T16:16:01.000Z', '1183416107878834179'],\n",
       " ['nigel_farage', '2019-10-11T15:45:16.000Z', '1182683593073713154'],\n",
       " ['nigel_farage', '2019-10-11T07:12:28.000Z', '1182554541591552000'],\n",
       " ['nigel_farage', '2019-10-10T20:51:47.000Z', '1182398341738631184'],\n",
       " ['nigel_farage', '2019-10-10T18:52:22.000Z', '1182368292113125376'],\n",
       " ['nigel_farage', '2019-10-10T17:03:01.000Z', '1182340773938913283'],\n",
       " ['nigel_farage', '2019-10-10T08:21:51.000Z', '1182209614336659456'],\n",
       " ['nigel_farage', '2019-10-09T20:41:10.000Z', '1182033283342839814'],\n",
       " ['nigel_farage', '2019-10-09T19:43:00.000Z', '1182018643909054464'],\n",
       " ['nigel_farage', '2019-10-09T17:06:38.000Z', '1181979293607514112'],\n",
       " ['nigel_farage', '2019-10-09T15:48:45.000Z', '1181959693012541441'],\n",
       " ['nigel_farage', '2019-10-09T14:26:35.000Z', '1181939018273038338'],\n",
       " ['nigel_farage', '2019-10-09T14:01:40.000Z', '1181932746555432962'],\n",
       " ['nigel_farage', '2019-10-09T10:26:57.000Z', '1181878709659324417'],\n",
       " ['nigel_farage', '2019-10-08T17:10:40.000Z', '1181617921291902977'],\n",
       " ['nigel_farage', '2019-10-08T14:59:12.000Z', '1181584834600275968'],\n",
       " ['nigel_farage', '2019-10-08T14:20:00.000Z', '1181574970746396673'],\n",
       " ['nigel_farage', '2019-10-08T13:41:27.000Z', '1181565270520975361'],\n",
       " ['nigel_farage', '2019-10-08T12:15:00.000Z', '1181543513734795265'],\n",
       " ['nigel_farage', '2019-10-08T11:30:00.000Z', '1181532188535328769'],\n",
       " ['nigel_farage', '2019-10-08T11:08:32.000Z', '1181526787068764161'],\n",
       " ['nigel_farage', '2019-10-08T10:51:04.000Z', '1181522391379394560'],\n",
       " ['nigel_farage', '2019-10-08T10:23:23.000Z', '1181515424267227136'],\n",
       " ['nigel_farage', '2019-10-08T10:03:58.000Z', '1181510537554857984'],\n",
       " ['nigel_farage', '2019-10-08T08:50:18.000Z', '1181491998659940352'],\n",
       " ['nigel_farage', '2019-10-07T19:20:53.000Z', '1181288302730780672'],\n",
       " ['nigel_farage', '2019-10-07T18:21:28.000Z', '1181273350687121411'],\n",
       " ['nigel_farage', '2019-10-07T17:05:35.000Z', '1181254256160104451'],\n",
       " ['nigel_farage', '2019-10-06T13:41:48.000Z', '1180840583516045312'],\n",
       " ['nigel_farage', '2019-10-06T13:10:03.000Z', '1180832592272199680'],\n",
       " ['nigel_farage', '2019-10-04T20:29:22.000Z', '1180218375034884096'],\n",
       " ['nigel_farage', '2019-10-04T18:39:20.000Z', '1180190682012622849'],\n",
       " ['nigel_farage', '2019-10-04T13:09:34.000Z', '1180107694327390213'],\n",
       " ['nigel_farage', '2019-10-03T14:57:56.000Z', '1179772577851547648'],\n",
       " ['nigel_farage', '2019-10-02T22:34:41.000Z', '1179525134245388288'],\n",
       " ['nigel_farage', '2019-10-02T18:40:27.000Z', '1179466190651084802'],\n",
       " ['nigel_farage', '2019-10-02T17:06:18.000Z', '1179442496985817090'],\n",
       " ['nigel_farage', '2019-10-02T15:24:46.000Z', '1179416942815465473'],\n",
       " ['nigel_farage', '2019-10-02T13:42:03.000Z', '1179391092057399297'],\n",
       " ['nigel_farage', '2019-10-02T11:04:54.000Z', '1179351544300494850'],\n",
       " ['nigel_farage', '2019-10-02T10:43:53.000Z', '1179346255526404096'],\n",
       " ['nigel_farage', '2019-10-01T17:07:04.000Z', '1179080301164793859']]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "%%time\n",
    "users_tweets('nigel_farage',\"2019-10-01T00:00:00.00Z\",\"2019-11-01T00:00:00.00Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "200\n",
      "Tweet 0 from 2019-11-01T01:51:01.000Z\n",
      "200\n",
      "Tweet 100 from 2019-10-06T23:15:00.000Z\n",
      "200\n",
      "Tweet 200 from 2019-10-04T16:49:58.000Z\n",
      "200\n",
      "Tweet 300 from 2019-10-03T13:25:11.000Z\n",
      "200\n",
      "Tweet 400 from 2019-10-01T22:39:27.000Z\n",
      "Wall time: 7.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CNNChile_tweets = users_tweets('CNNChile',\"2019-10-01T00:00:00.00Z\",\"2019-10-07T00:00:00.00Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get all tweet ID's from a given date range for a list of users\n",
    "\n",
    "def get_list_users_tweets(users, start_date, end_date):\n",
    "    '''\n",
    "    users - a list of twitter user\n",
    "    start_date - start date + time we want\n",
    "    end_date - end date + time we want\n",
    "    '''\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for user in users:\n",
    "        \n",
    "        print('-'*30)\n",
    "        print(f\"User: {user} Number: {users.index(user)+1} of {len(users)}\")\n",
    "        print('-'*30)\n",
    "        print()\n",
    "        # Append results of function to results\n",
    "        # Extend will insure everying is in the same list\n",
    "        # Append would create a list of lists\n",
    "        res.extend( users_tweets(user, start_date, end_date ) )\n",
    "\n",
    "    #Convert results from list of list to DataFrame\n",
    "    tweets_df = pd.DataFrame.from_records(res, columns=['User', 'TweetCreated', 'TweetId'])\n",
    "\n",
    "    return tweets_df\n"
   ]
  },
  {
   "source": [
    "# Chilian News sources\n",
    "\n",
    "### So far these are the sources that have been identified as potentially interesting:\n",
    "\n",
    "- El Mercurio (@ElMercurio_cl) / Twitter\n",
    "- La Tercera (@latercera) / Twitter\n",
    "- laSegunda (@La_Segunda) / Twitter\n",
    "- Las Últimas Noticias (@lun) / Twitter\n",
    "- La Cuarta (@lacuarta) / Twitter\n",
    "- Meganoticias (@meganoticiascl) / Twitter\n",
    "- Canal 13 (@canal13) / Twitter\n",
    "- TVN (@TVN) / Twitter\n",
    "- 24 Horas (@24HorasTVN) / Twitter\n",
    "- La Nación Chile (@nacioncl) / Twitter\n",
    "- Diario Financiero (@DFinanciero) / Twitter\n",
    "- Chilevisión (@chilevision) / Twitter\n",
    "- UCV Radio 103.5 FM (@ucvradio) / Twitter\n",
    "- LaRed (@LaRedTV) / Twitter\n",
    "- Cooperativa (@Cooperativa) / Twitter\n",
    "- Radio Pudahuel (@RadioPudahuel) / Twitter\n",
    "- BioBioChile (@biobio) / Twitter\n",
    "- El Conquistador 91.3 (@FMConquistador) / Twitter\n",
    "- EL INFORMADORCHILE🇨🇱 (@INFORMADORCHILE) / Twitter\n",
    "- CNN Chile (@CNNChile) / Twitter\n",
    "- CHV Noticias (@CHVNoticias) / Twitter\n",
    "- El Mostrador (@elmostrador) / Twitter\n",
    "- Publimetro (@PublimetroChile) / Twitter\n",
    "- El Desconcierto (@eldesconcierto) / Twitter\n",
    "- El Dínamo (@el_dinamo) / Twitter\n",
    "- El Ciudadano (@El_Ciudadano) / Twitter\n",
    "- El Líbero (@elliberocl) / Twitter\n",
    "- (4) Diario El Observador (@eo_enlinea) / Twitter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In terms of the time period, if you could look between October 18, 2019 and November 18, 2019, that would be some cool gravy.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------\n",
      "User: ElMercurio_cl Number: 1 of 1\n",
      "------------------------------\n",
      "\n",
      "200\n",
      "Tweet 0 from 2019-11-01T02:50:00.000Z\n",
      "200\n",
      "Tweet 100 from 2019-10-30T18:00:00.000Z\n",
      "200\n",
      "Tweet 200 from 2019-10-28T20:00:00.000Z\n",
      "200\n",
      "Tweet 300 from 2019-10-27T12:50:00.000Z\n",
      "200\n",
      "Tweet 400 from 2019-10-25T00:00:00.000Z\n",
      "200\n",
      "Tweet 500 from 2019-10-22T21:00:00.000Z\n",
      "200\n",
      "Tweet 600 from 2019-10-20T16:03:14.000Z\n",
      "200\n",
      "Tweet 700 from 2019-10-18T17:00:01.000Z\n",
      "200\n",
      "Tweet 800 from 2019-10-16T20:00:00.000Z\n",
      "200\n",
      "Tweet 900 from 2019-10-14T22:20:00.000Z\n",
      "200\n",
      "Tweet 1000 from 2019-10-13T10:30:00.000Z\n",
      "200\n",
      "Tweet 1100 from 2019-10-11T00:30:00.000Z\n",
      "200\n",
      "Tweet 1200 from 2019-10-09T10:30:00.000Z\n",
      "200\n",
      "Tweet 1300 from 2019-10-07T12:40:00.000Z\n",
      "200\n",
      "Tweet 1400 from 2019-10-05T12:45:00.000Z\n",
      "200\n",
      "Tweet 1500 from 2019-10-03T13:00:00.000Z\n",
      "200\n",
      "Tweet 1600 from 2019-10-01T14:00:00.000Z\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chile_news_users = ['ElMercurio_cl',\n",
    "                    'latercera',\n",
    "                    'La_Segunda',\n",
    "                    'lun',\n",
    "                    'lacuarta',\n",
    "                    'meganoticiascl',\n",
    "                    'canal13',\n",
    "                    'TVN',\n",
    "                    '24HorasTVN',\n",
    "                    'nacioncl',\n",
    "                    'DFinanciero',\n",
    "                    'chilevision',\n",
    "                    'ucvradio',\n",
    "                    'LaRedTV',\n",
    "                    'Cooperativa',\n",
    "                    'RadioPudahuel',\n",
    "                    'biobio',\n",
    "                    'FMConquistador',\n",
    "                    'INFORMADORCHILE',\n",
    "                    'CNNChile',\n",
    "                    'CHVNoticias',\n",
    "                    'elmostrador',\n",
    "                    'PublimetroChile',\n",
    "                    'eldesconcierto',\n",
    "                    'el_dinamo',\n",
    "                    'El_Ciudadano',\n",
    "                    'elliberocl',\n",
    "                    'eo_enlinea']\n",
    "\n",
    "start_date = \"2019-10-01T00:00:00.00Z\"\n",
    "end_date = \"2019-11-01T00:00:00.00Z\"\n",
    "\n",
    "# Try getting just the dates we want in 2019\n",
    "results = get_list_users_tweets(chile_news_users[:1], start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               User              TweetCreated              TweetId\n",
       "0     ElMercurio_cl  2019-11-01T02:50:00.000Z  1190098635121283072\n",
       "1     ElMercurio_cl  2019-11-01T02:40:00.000Z  1190096118329135104\n",
       "2     ElMercurio_cl  2019-11-01T02:30:00.000Z  1190093602464129024\n",
       "3     ElMercurio_cl  2019-11-01T02:20:00.000Z  1190091085407825921\n",
       "4     ElMercurio_cl  2019-11-01T02:10:00.000Z  1190088568707923969\n",
       "...             ...                       ...                  ...\n",
       "1619  ElMercurio_cl  2019-10-01T01:00:00.000Z  1178836929401561089\n",
       "1620  ElMercurio_cl  2019-10-01T00:45:00.000Z  1178833153995235328\n",
       "1621  ElMercurio_cl  2019-10-01T00:30:00.000Z  1178829379150827520\n",
       "1622  ElMercurio_cl  2019-10-01T00:15:00.000Z  1178825604273250304\n",
       "1623  ElMercurio_cl  2019-10-01T00:00:00.000Z  1178821829990862848\n",
       "\n",
       "[1624 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User</th>\n      <th>TweetCreated</th>\n      <th>TweetId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-11-01T02:50:00.000Z</td>\n      <td>1190098635121283072</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-11-01T02:40:00.000Z</td>\n      <td>1190096118329135104</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-11-01T02:30:00.000Z</td>\n      <td>1190093602464129024</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-11-01T02:20:00.000Z</td>\n      <td>1190091085407825921</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-11-01T02:10:00.000Z</td>\n      <td>1190088568707923969</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1619</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-10-01T01:00:00.000Z</td>\n      <td>1178836929401561089</td>\n    </tr>\n    <tr>\n      <th>1620</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-10-01T00:45:00.000Z</td>\n      <td>1178833153995235328</td>\n    </tr>\n    <tr>\n      <th>1621</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-10-01T00:30:00.000Z</td>\n      <td>1178829379150827520</td>\n    </tr>\n    <tr>\n      <th>1622</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-10-01T00:15:00.000Z</td>\n      <td>1178825604273250304</td>\n    </tr>\n    <tr>\n      <th>1623</th>\n      <td>ElMercurio_cl</td>\n      <td>2019-10-01T00:00:00.000Z</td>\n      <td>1178821829990862848</td>\n    </tr>\n  </tbody>\n</table>\n<p>1624 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}