{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Archive Search\n",
    "\n",
    "- Coppying form this github page https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Full-Archive-Search/full-archive-search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Import key from environment variable to keep my credentials secret\n",
    "bearer_token = os.environ[\"twitter_bearer_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up functions needed for search\n",
    "\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        print(response.status_code)\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn into function for a given user\n",
    "\n",
    "Got it to work here, try to functionise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all results from one user\n",
    "\n",
    "# TODO: Documentation\n",
    "def users_tweets(user, start_time, end_time):\n",
    "    '''\n",
    "    users - a list of twitter user\n",
    "    start_time - start date + time we want #TODO: Implement\n",
    "    end_time - end date + time we want   #TODO: Implement\n",
    "    '''\n",
    "    num_runs = 0\n",
    "    all_results = []\n",
    "\n",
    "    result_count = 100  # initilise this as a non-zero number\n",
    "                        # will be filled with respnses from json later \n",
    "                        # can get a max of 100 records at a time\n",
    "                        # so if the results arnt 100 we ran out of things to get\n",
    "\n",
    "    while result_count == 100:\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        if num_runs ==0:\n",
    "            # No next_token parameter\n",
    "            query_params = { #\"query\":\"from:CNNChile lang:es\",\n",
    "                    'query': f'(from:{user} -is:retweet) ',\n",
    "                    'tweet.fields': 'author_id,id,created_at',\n",
    "                    \"start_time\":\"2019-10-01T07:20:50.52Z\",\n",
    "                    \"end_time\":\"2019-11-01T07:20:50.52Z\",\n",
    "                    \"max_results\": \"100\"\n",
    "            }\n",
    "        else:\n",
    "            # with next_token\n",
    "            query_params = { #\"query\":\"from:CNNChile lang:es\",\n",
    "                    'query': f'(from:{user} -is:retweet) ',\n",
    "                    'tweet.fields': 'author_id,id,created_at',\n",
    "                    \"start_time\":f\"{start_time}\",\n",
    "                    \"end_time\":f\"{end_time}\",\n",
    "                    \"max_results\": \"100\",\n",
    "                    \"next_token\": results_json['meta']['next_token'] # get token from before: 'b26v89c19zqg8o3fnm91i40gu1gp7vh7j0r6ddrjyhvul'\n",
    "            }\n",
    "\n",
    "        headers = create_headers(bearer_token)\n",
    "        json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "        results = json.dumps(json_response, indent=4, sort_keys=True)\n",
    "\n",
    "        results_json = json.loads(results)\n",
    "\n",
    "        # append this pagnation to all results\n",
    "        #all_results.append(results_json)\n",
    "        result_count = results_json['meta']['result_count']\n",
    "        num_runs = num_runs + 1\n",
    "        #print(num_runs, result_count)\n",
    "\n",
    "        # Only add results if theyre non zero\n",
    "        if result_count > 0:\n",
    "            list_of_tweet_dicts = results_json['data']\n",
    "\n",
    "            print(f\"Tweet {len(all_results)} from {list_of_tweet_dicts[0]['created_at']}\")\n",
    "\n",
    "            for tweet in list_of_tweet_dicts:\n",
    "                all_results.append([user, tweet['created_at'], tweet['id']])\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "users_tweets('nigel_farage',\"2019-10-01T00:00:00.00Z\",\"2019-11-01T00:00:00.00Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "CNNChile_tweets = users_tweets('CNNChile',\"2019-10-01T00:00:00.00Z\",\"2019-10-07T00:00:00.00Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get all tweet ID's from a given date range for a list of users\n",
    "\n",
    "def get_list_users_tweets(users, start_date, end_date):\n",
    "    '''\n",
    "    users - a list of twitter user\n",
    "    start_date - start date + time we want\n",
    "    end_date - end date + time we want\n",
    "    '''\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for user in users:\n",
    "        \n",
    "        print('-'*30)\n",
    "        print(f\"User: {user} Number: {users.index(user)+1} of {len(users)}\")\n",
    "        print('-'*30)\n",
    "        print()\n",
    "        # Append results of function to results\n",
    "        # Extend will insure everying is in the same list\n",
    "        # Append would create a list of lists\n",
    "        res.extend( users_tweets(user, start_date, end_date ) )\n",
    "\n",
    "    #Convert results from list of list to DataFrame\n",
    "    tweets_df = pd.DataFrame.from_records(res, columns=['User', 'TweetCreated', 'TweetId'])\n",
    "\n",
    "    return tweets_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chilian News sources\n",
    "\n",
    "### So far these are the sources that have been identified as potentially interesting:\n",
    "\n",
    "- El Mercurio (@ElMercurio_cl) / Twitter\n",
    "- La Tercera (@latercera) / Twitter\n",
    "- laSegunda (@La_Segunda) / Twitter\n",
    "- Las Ãšltimas Noticias (@lun) / Twitter\n",
    "- La Cuarta (@lacuarta) / Twitter\n",
    "- Meganoticias (@meganoticiascl) / Twitter\n",
    "- Canal 13 (@canal13) / Twitter\n",
    "- TVN (@TVN) / Twitter\n",
    "- 24 Horas (@24HorasTVN) / Twitter\n",
    "- La NaciÃ³n Chile (@nacioncl) / Twitter\n",
    "- Diario Financiero (@DFinanciero) / Twitter\n",
    "- ChilevisiÃ³n (@chilevision) / Twitter\n",
    "- UCV Radio 103.5 FM (@ucvradio) / Twitter\n",
    "- LaRed (@LaRedTV) / Twitter\n",
    "- Cooperativa (@Cooperativa) / Twitter\n",
    "- Radio Pudahuel (@RadioPudahuel) / Twitter\n",
    "- BioBioChile (@biobio) / Twitter\n",
    "- El Conquistador 91.3 (@FMConquistador) / Twitter\n",
    "- EL INFORMADORCHILEðŸ‡¨ðŸ‡± (@INFORMADORCHILE) / Twitter\n",
    "- CNN Chile (@CNNChile) / Twitter\n",
    "- CHV Noticias (@CHVNoticias) / Twitter\n",
    "- El Mostrador (@elmostrador) / Twitter\n",
    "- Publimetro (@PublimetroChile) / Twitter\n",
    "- El Desconcierto (@eldesconcierto) / Twitter\n",
    "- El DÃ­namo (@el_dinamo) / Twitter\n",
    "- El Ciudadano (@El_Ciudadano) / Twitter\n",
    "- El LÃ­bero (@elliberocl) / Twitter\n",
    "- (4) Diario El Observador (@eo_enlinea) / Twitter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In terms of the time period, if you could look between October 18, 2019 and November 18, 2019, that would be some cool gravy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "chile_news_users = ['ElMercurio_cl',\n",
    "                    'latercera',\n",
    "                    'La_Segunda',\n",
    "                    'lun',\n",
    "                    'lacuarta',\n",
    "                    'meganoticiascl',\n",
    "                    'canal13',\n",
    "                    'TVN',\n",
    "                    '24HorasTVN',\n",
    "                    'nacioncl',\n",
    "                    'DFinanciero',\n",
    "                    'chilevision',\n",
    "                    'ucvradio',\n",
    "                    'LaRedTV',\n",
    "                    'Cooperativa',\n",
    "                    'RadioPudahuel',\n",
    "                    'biobio',\n",
    "                    'FMConquistador',\n",
    "                    'INFORMADORCHILE',\n",
    "                    'CNNChile',\n",
    "                    'CHVNoticias',\n",
    "                    'elmostrador',\n",
    "                    'PublimetroChile',\n",
    "                    'eldesconcierto',\n",
    "                    'el_dinamo',\n",
    "                    'El_Ciudadano',\n",
    "                    'elliberocl',\n",
    "                    'eo_enlinea']\n",
    "\n",
    "start_date = \"2019-10-01T00:00:00.00Z\"\n",
    "end_date = \"2019-11-01T00:00:00.00Z\"\n",
    "\n",
    "# Try getting just the dates we want in 2019\n",
    "results = get_list_users_tweets(chile_news_users, start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('reults.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['User'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('twitter_scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "671e37286a88e4deab955730fbc96219c4d8a59a405d8ba3190d5d9a90bdb905"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
